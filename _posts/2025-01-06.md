
## Monte Carlo


Monte Carlo integration estimates the expected value of a function  $\mathbb{E}[f(X)] = \int f(x)p(x)dx$ , where $ p(x)$  is the target distribution. In low dimensions, numerical methods like quadrature (1D) or cubature (higher dimensions) can compute this integral, but they fail to scale in higher dimensions. Monte Carlo integration overcomes this by drawing  $N_s$  random samples  $x_n \sim p(x)$  and approximating:

$\mathbb{E}[f(X)] \approx \frac{1}{N_s} \sum_{n=1}^{N_s} f(x_n).$
  
This method evaluates the function only in regions of significant probability, making it efficient in high dimensions. Its accuracy depends only on the number of samples  $N_s$  and is independent of the dimensionality of  $x$ , though the estimator can have high variance. Generating samples  $x_n \sim p(x)$  efficiently remains a challenge in practice.

Looking at the estimator in MC, we have 

$\hat{\mu} = \frac{1}{N} \sum_{i=1}^N f(X_i),$
  
where $X_i \sim p(x)$ are i.i.d. samples. This estimator is **unbiasedness**, meaning that the expected value of $\hat{\mu}$ is

$\mathbb{E}[\hat{\mu}] = \mu,$
  
making $\hat{\mu}$ an unbiased estimator. Additionally, we can find the **variance of** $\hat{\mu}$,  since the variance of the sum of i.i.d. random variables scales with $1/N^2$, the variance of $\hat{\mu}$ is

$\text{Var}(\hat{\mu}) = \frac{\sigma^2}{N},$
  
where $\sigma^2 = \text{Var}(f(X))$.

4. **Central Limit Theorem**: By the CLT, the normalized sum converges to a normal distribution:

\[

$\sqrt{N} (\hat{\mu} - \mu) \xrightarrow{d} \mathcal{N}(0, \sigma^2).$
\]

5. **Final Result**: Dividing by \sqrt{N}, the difference (\hat{\mu} - \mu) follows:

  

\hat{\mu} - \mu \sim \mathcal{N}\left(0, \frac{\sigma^2}{N}\right).