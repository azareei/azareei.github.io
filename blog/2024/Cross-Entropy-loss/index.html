<!DOCTYPE html>
<html>

<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Ahmad  Zareei | Cross Entropy Loss Derivation</title>
<meta name="description" content="Personal homepage of Ahmad Zareei, Senion AI research Scientist @META
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous">

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://combinatronics.io/jwarby/pygments-css/master/github.css">

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/logo.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/blog/2024/Cross-Entropy-loss/">

<!-- Theming-->

<script src="/assets/js/theme.js"></script>



  
<!-- MathJax -->
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            processEscapes: true,
            processEnvironments: true
        },
        options: {
            skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            ignoreHtmlClass: 'tex2jax_ignore',
            processHtmlClass: 'tex2jax_process'
        }
    };
</script>

  <meta name="google-site-verification" content="SwVgS1KjZmScXzgcGAsFKN5eksnUe79r12brKRMZLko">
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-TWETJLDW9Z"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-TWETJLDW9Z');
  </script>
</head>

<body class="fixed-top-nav ">

  <!-- Header -->

  <header>

  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="https://azareei.github.io/">
        </a><a href="https://azareei.github.io/" class="fab"> <img style="height: 27px; width: auto; vertical-align: middle;padding-right: 10px;" src="/assets/img/logo.png" class="ai" alt="AZ">
        </a>
        <span class="font-weight-bold">Ahmad </span>    Zareei
      
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          <!-- Other pages  | sort: "title" -->
          
          
          
          
          
          
          
          <li class="nav-item ">
            <a class="nav-link" href="/publications/">
              publications
              
            </a>
          </li>
          
          
          
          <li class="nav-item ">
            <a class="nav-link" href="/projects/">
              research
              
            </a>
          </li>
          
          
          
          
          
          
          
          
          
          
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          
          <div class="toggle-container">
            <a id="light-toggle">
              <i class="fas fa-moon"></i>
              <i class="fas fa-sun"></i>
            </a>
          </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>

  <!-- Content -->

  <div class="container mt-5">
    

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Cross Entropy Loss Derivation</h1>
    <p class="post-meta">February 14, 2024</p>
  </header>

  <article class="post-content">
    <p><a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html" rel="external nofollow noopener" target="_blank">Cross entropy loss</a> is defined as</p>

\[L = \frac{1}{N} \sum_{n=1}^N l_n, \qquad l_n = -  \frac{\exp(x_{n,c})}{\sum_i \exp(x_{n,i})}, \text{where } y_n=c\]

<p>where \(x\) is the input, \(y\) is the target, \(C\) is the number of classes, \(N\) is the mini-batch size. So the network for each case of \(n\), predicts values of \([x_{n,1}, ..., x_{n,C}]^T\), and we pass it to log-soft-max, and then depending on what class it belongs to \(y_{n,c}\) use that value, and then average over all the cases of mini-batch \(n=1, ..., N\).  Note that \(y_{n}\) here represents the group that it belongs to, in terms of one-hot vector it can also be written as</p>

\[L = \frac{1}{N} \sum_{n=1}^N l_n, \qquad l_n = -  \sum_{c=1}^C \frac{\exp(x_{n,c})}{\sum_i \exp(x_{n,i})} y_{n,c}\]

<p>where here we used the one-hot representation of \(y_n\).</p>

<p>Our goal here is to do a derivation, to show why is the cross-entropy loss is defined as above.</p>

<p>The Kullback-Leibler (KL) divergence between the two probability distribution \(q(z)\) and \(p(z)\) is defined as</p>

\[D_{KL}[q||p] = \int_{-\infty}^{\infty} q(z) \log \frac{q(z)}{p(z)} dz\]

<p>Now consider that we observe an empirical data \(\{y_i\}_{i=1}^{N}\) (which are the classes for each case of the data). We can consider the output distribution is a weighted sum of the point masses as</p>

\[q(y) = \frac{1}{N} \sum_{i=1}^N \delta (y-y_{i})\]

<p>where \(\delta(\cdot)\) is the delta Dirac function. We want to minimize the KL divergence between the output of the neural network 
\(P(y|\theta)\), 
and this empirical distribution,</p>

\[\hat{\theta} = \arg\min_\theta \left[ \int_{-\infty}^{\infty} q(y) \log {q(y)} dy - \int_{-\infty}^{\infty} q(y) \log {p(y)} dy \right]\]

\[\hat{\theta} = - \arg\min_\theta  \int_{-\infty}^{\infty} q(y) \log {P(y|\theta)} dy\]

<p>Now, we replace for \(q(y)\) to find</p>

\[\hat{\theta} = - \arg\min_\theta \int_{-\infty}^{\infty} \left( \frac{1}{N} \sum_{n=1}^N \delta (y-y_{n}) \right)  \log {P(y|\theta)} dy\]

\[\hat{\theta} = - \arg\min_\theta \frac{1}{N}  \sum_{n=1}^N \log {P(y_n|\theta)}\]

<p>Note that the output of the network is  \([x_{n,1}, ..., x_{n,C}]^T\), that is transformed into probabilities using a soft-max function as</p>

\[P(y_n|\theta) = \sum_{c=1}^C \frac{\exp(x_{n,c})}{\sum_i \exp(x_{n,i})} y_{n,c}\]

<p>So as can be seen above the loss can be written as</p>

\[L = \frac{1}{N} \sum_{n=1}^N l_n, \qquad l_n = -  \sum_{c=1}^C \frac{\exp(x_{n,c})}{\sum_i \exp(x_{n,i})} y_{n,c}\]

<p>So that’s it. Basically cross-entropy loss is the KL divergence between the point-mass distribution and the output probability prediction of the network (using a soft-max probability assignment).</p>

  </article>

  

</div>

  </div>

  <!-- Footer -->

  
<footer class="fixed-bottom">
  <div class="container mt-0" style="text-align:center;">
    © Copyright 2025 Ahmad  Zareei.
    <!--  -->     
    
    Last updated: January 14, 2025.
    
  </div>
</footer>



</body>

<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

<!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>


<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>





<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html><html>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-77789841-1', 'auto');
  ga('send', 'pageview');

</script>
</html>
